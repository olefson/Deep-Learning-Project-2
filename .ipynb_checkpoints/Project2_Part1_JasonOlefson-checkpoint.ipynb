{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Jason Olefson Project 2 Part 1 Deep Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>GPU Check</h1>\n",
    "<h3>IMPORTANT In this project, I used my GPU (NVIDIA RTX 4080) to train my model. Because of this, you may need to configure the first few blocks on this file to run on your device. Thank you.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14943408952912050139\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 14048821248\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14616454465771647725\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:01:00.0, compute capability: 8.9\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# # Check if TensorFlow is using GPU for cuDNN-enabled operations\n",
    "# print(\"Is TensorFlow using GPU?: \", tf.test.is_built_with_cuda())\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and being used.\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available and being used.\")\n",
    "else:\n",
    "    print(\"No GPU detected, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Prep</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baby_ID</th>\n",
       "      <th>Heart_Rate</th>\n",
       "      <th>Respiratory_Rate</th>\n",
       "      <th>Oxygen_Saturation</th>\n",
       "      <th>Pain_Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>60</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "      <td>60</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>142</td>\n",
       "      <td>60</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>142</td>\n",
       "      <td>59</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>143</td>\n",
       "      <td>59</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Baby_ID  Heart_Rate  Respiratory_Rate  Oxygen_Saturation Pain_Level\n",
       "0        0         142                60              100.0          0\n",
       "1        1         142                60              100.0          0\n",
       "2        2         142                60              100.0          0\n",
       "3        3         142                59              100.0          0\n",
       "4        4         143                59              100.0          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define data directory path\n",
    "data_dir = \"./Data/\"\n",
    "\n",
    "data_frames = [] # empty list to hold DataFrames\n",
    "\n",
    "# Combine data into single DataFrame\n",
    "# loop over each subfolder\n",
    "for subfolder in os.listdir(data_dir):\n",
    "    subfolder_path = os.path.join(data_dir, subfolder)\n",
    "    # check if directory\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # loop over all csv\n",
    "        for file in os.listdir(subfolder_path):\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(subfolder_path, file)\n",
    "                df = pd.read_csv(file_path, header=None) #read csv\n",
    "                data_frames.append(df) # add Datafram to list\n",
    "combined_data = pd.concat(data_frames, ignore_index=True) #combine into single DataFrame\n",
    "combined_data.columns = [\"Baby_ID\", \"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\", \"Pain_Level\"] # rename columns for clarity\n",
    "combined_data = combined_data[combined_data[\"Pain_Level\"] != \"#\"] # remove rows with \"#\" in the 4th column\n",
    "# Remove the pesky typo\n",
    "combined_data[\"Heart_Rate\"] = pd.to_numeric(combined_data[\"Heart_Rate\"], errors='coerce')\n",
    "combined_data[\"Respiratory_Rate\"] = pd.to_numeric(combined_data[\"Respiratory_Rate\"], errors='coerce')\n",
    "combined_data[\"Oxygen_Saturation\"] = pd.to_numeric(combined_data[\"Oxygen_Saturation\"], errors='coerce')\n",
    "combined_data.dropna(subset=[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"], inplace=True)\n",
    "combined_data.head() # for clarity (display first few columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Shuffle/Split Dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = shuffle(combined_data, random_state=42) # shuffle dataset\n",
    "train, temp = train_test_split(combined_data, test_size=0.1, random_state=42) # split to training/temp sets (90% training, 10% temp)\n",
    "validation, test = train_test_split(temp, test_size=0.5, random_state=42) # split temp set into validation/test sets (10% of total each)\n",
    "\n",
    "# separate features/labels for each set\n",
    "X_train = train[[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"]]\n",
    "y_train = train[\"Pain_Level\"]\n",
    "X_val = validation[[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"]]\n",
    "y_val = validation[\"Pain_Level\"]\n",
    "X_test = test[[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"]]\n",
    "y_test = test[\"Pain_Level\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Normalization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Heart_Rate</th>\n",
       "      <th>Respiratory_Rate</th>\n",
       "      <th>Oxygen_Saturation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.008015</td>\n",
       "      <td>1.379319</td>\n",
       "      <td>0.346797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.179711</td>\n",
       "      <td>-0.030464</td>\n",
       "      <td>0.346797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.254802</td>\n",
       "      <td>1.072845</td>\n",
       "      <td>0.272768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.008015</td>\n",
       "      <td>0.521190</td>\n",
       "      <td>0.346797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.777694</td>\n",
       "      <td>-1.440248</td>\n",
       "      <td>0.346797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Heart_Rate  Respiratory_Rate  Oxygen_Saturation\n",
       "0   -0.008015          1.379319           0.346797\n",
       "1    0.179711         -0.030464           0.346797\n",
       "2    0.254802          1.072845           0.272768\n",
       "3   -0.008015          0.521190           0.346797\n",
       "4   -0.777694         -1.440248           0.346797"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalization for feature columns\n",
    "X_train = scaler.fit_transform(train[[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"]])\n",
    "X_val = scaler.transform(validation[[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"]])\n",
    "X_test = scaler.transform(test[[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"]])\n",
    "# Convert normalized arrays back to DataFrames\n",
    "X_train = pd.DataFrame(X_train, columns=[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"])\n",
    "X_val = pd.DataFrame(X_val, columns=[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"])\n",
    "X_test = pd.DataFrame(X_test, columns=[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"])\n",
    "# Label extraction\n",
    "y_train = train[\"Pain_Level\"].reset_index(drop=True)\n",
    "y_val = validation[\"Pain_Level\"].reset_index(drop=True)\n",
    "y_test = test[\"Pain_Level\"].reset_index(drop=True)\n",
    "\n",
    "X_train.head() # for clarity (display first few columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature data: Convert to float32 and reshape to (samples, timesteps, features)\n",
    "X_train = X_train.values.astype('float32').reshape(-1, 3, 1)\n",
    "X_val = X_val.values.astype('float32').reshape(-1, 3, 1)\n",
    "X_test = X_test.values.astype('float32').reshape(-1, 3, 1)\n",
    "\n",
    "# Label data: Convert to int32 without reshaping further\n",
    "y_train = y_train.astype('int32')\n",
    "y_val = y_val.astype('int32')\n",
    "y_test = y_test.astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Build GRU Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 3, 16)             912       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3, 16)             0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 3, 16)             1632      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 3, 16)             0         \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 3, 16)             1632      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 3, 16)             0         \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 3, 16)             1632      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 3, 16)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3, 3)              51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,859\n",
      "Trainable params: 5,859\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Architecture\n",
    "model = Sequential([\n",
    "    GRU(16, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "    Dropout(0.2),\n",
    "    GRU(16, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    GRU(16, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    GRU(16, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation='softmax') # 3 unit output layer (for 3 classes) / softmax activation\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary() # display summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert' defined at (most recent call last):\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_40624\\105791781.py\", line 6, in <module>\n      history = model.fit(\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\losses.py\", line 2084, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend.py\", line 5630, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert'\nassertion failed: [Condition x == y did not hold element-wise:] [x (sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/Shape_1:0) = ] [32 1] [y (sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/strided_slice:0) = ] [32 3]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert}}]] [Op:__inference_train_function_10711]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert' defined at (most recent call last):\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_40624\\105791781.py\", line 6, in <module>\n      history = model.fit(\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\losses.py\", line 2084, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend.py\", line 5630, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert'\nassertion failed: [Condition x == y did not hold element-wise:] [x (sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/Shape_1:0) = ] [32 1] [y (sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/strided_slice:0) = ] [32 3]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert}}]] [Op:__inference_train_function_10711]"
     ]
    }
   ],
   "source": [
    "# Epochs/Batch size (adjust as needed)\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Plot Training and Valid Performance</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Validation Accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Training/Validation Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
